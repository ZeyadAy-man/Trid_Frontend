import React, { useState, useEffect, useRef } from 'react';
import { useGLTF, useAnimations } from '@react-three/drei';

const VoiceAssistant = () => {
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [conversation, setConversation] = useState([]);
  const groupRef = useRef();

  const ELEVENLABS_API_KEY = 'sk_2fc7aca39c4ec0fe92fba7f62e1ce53ebbd8932ba827d9e5';
  const OPENROUTER_API_KEY = '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';

  const { scene, animations } = useGLTF('../../Assets/3D_Models/Lady/scene1.glb');
  const { actions, names } = useAnimations(animations, groupRef);

  useEffect(() => {
    if (!actions || !names) return;
    const idleAction = actions.Idle || actions[names[0]];
    const talkAction = actions.Talk || actions[names[1]] || actions[names[0]];
    if (isListening || isProcessing) {
      idleAction?.fadeOut(0.3);
      talkAction?.reset().fadeIn(0.3).play();
    } else {
      talkAction?.fadeOut(0.3);
      idleAction?.reset().fadeIn(0.3).play();
    }
    return () => {
      names.forEach(name => actions[name]?.stop());
    };
  }, [isListening, isProcessing, actions, names]);

  const queryAI = async (inputText) => {
    setIsProcessing(true);
    try {
      const messages = [
        {
          role: "system",
          content: `Ø£Ù†ØªÙ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø°ÙƒÙŠØ© Ø¯Ø§Ø®Ù„ Ù…ØªØ¬Ø± Ø´Ù†Ø· ÙØ§Ø®Ø± ÙÙŠ Ù…ÙˆÙ„ TriD. Ø£Ø¬ÙŠØ¨ÙŠ Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆÙˆØ¯ØŒ ÙˆØ³Ø§Ø¹Ø¯ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ø§Ø®ØªÙŠØ§Ø± ÙˆØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø´Ù†Ø· ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª. Ø¥Ù„ÙŠÙƒ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ØªØ¬Ø±:

ðŸ‘œ Ø§Ù„Ø±Ù Ø§Ù„Ø¹Ù„ÙˆÙŠ ÙŠØ³Ø§Ø±:
- Christian DIOR (Ø¨ÙŠØ¬ ÙƒØ¨ÙŠØ±) - $1100
- DIOR (Ø±Ù…Ø§Ø¯ÙŠ Ù…ØªØ¯Ø§Ø®Ù„ ÙƒØ¨ÙŠØ±) - $1200
- DIOR (Ø£Ø­Ù…Ø± ÙˆØ£Ø³ÙˆØ¯ ÙƒØ¨ÙŠØ±) - $1550

ðŸ‘œ Ø§Ù„Ø±Ù Ø§Ù„Ø³ÙÙ„ÙŠ ÙŠØ³Ø§Ø±:
- Hermes (Ø±Ù…Ø§Ø¯ÙŠ Ù…ØªØ¯Ø§Ø®Ù„ ÙƒØ¨ÙŠØ±) - $970
- Rose Bag (Ø£Ø­Ù…Ø± Ù…Ø¹ Ù†Ù…ÙˆØ± ÙƒØ¨ÙŠØ±) - $1000
- DIOR (Ø¨Ù†ÙŠ ÙƒØ¨ÙŠØ±) - $1200
- DIOR (Ø£Ø­Ù…Ø± ÙˆØ£Ø³ÙˆØ¯ ÙƒØ¨ÙŠØ±) - $1500

ðŸ‘œ Ø§Ù„ØªØ±Ø§Ø¨ÙŠØ²Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ†:
- GUCCI (Ø±Ù…Ø§Ø¯ÙŠ Ù…ØªØ¯Ø§Ø®Ù„ ÙƒØ¨ÙŠØ±) - $1700
- Prada (Ø£Ø­Ù…Ø± Ù…Ø¹ Ù†Ù…ÙˆØ± ÙƒØ¨ÙŠØ±) - $1400
- Christian DIOR (Ø¨ÙŠØ¬ ÙƒØ¨ÙŠØ±) - $1100

ðŸ‘œ Ø§Ù„ØªØ±Ø§Ø¨ÙŠØ²Ø© Ø£Ù…Ø§Ù…Ùƒ:
- Flower bag (Ø£Ø®Ø¶Ø± ÙˆØ£Ø¨ÙŠØ¶ ÙˆØ±Ø¯ÙŠ ØµØºÙŠØ±) - $920
- Dior (Ø£Ø²Ø±Ù‚ ØµØºÙŠØ±) - $1200
- Louis Vuitton (Ø£Ø­Ù…Ø± ØµØºÙŠØ±) - $1400

ðŸ¬ Ø£Ù‚Ø³Ø§Ù… Ø£Ø®Ø±Ù‰:
- Ø£Ø­Ø°ÙŠØ©: Ø§Ù„Ø·Ø§Ø¨Ù‚ Ø§Ù„Ø«Ø§Ù†ÙŠ
- Ù…Ù„Ø§Ø¨Ø³: Ø§Ù„Ø·Ø§Ø¨Ù‚ Ø§Ù„Ø£ÙˆÙ„
- Ø±ÙŠØ§Ø¶Ø©: Ø§Ù„Ø·Ø§Ø¨Ù‚ Ø§Ù„Ø«Ø§Ù„Ø«

ðŸŽ¨ Ù†ØµØ§Ø¦Ø­ ØªÙ†Ø³ÙŠÙ‚:
- Ø§Ù„Ø¨Ù„ÙŠØ²Ø± Ø§Ù„Ø£Ø³ÙˆØ¯ ÙŠÙ„ÙŠÙ‚ Ø¹Ù„ÙŠÙ‡ Ø¬Ø²Ù…Ø© Ø³ÙˆØ¯Ø§Ø¡ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø¨Ù†ÙŠ
- Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„Ø¨ÙŠØ¬ ÙŠÙ†Ø§Ø³Ø¨Ù‡ Ø´Ù†Ø·Ø© Ø±Ù…Ø§Ø¯ÙŠØ© Ø£Ùˆ ÙˆØ±Ø¯ÙŠØ©

Ø¯Ø§Ø¦Ù…Ù‹Ø§ ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø©ØŒ Ù…Ø®ØªØµØ±Ø©ØŒ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ©.`
        },
        ...conversation.map(msg => ({
          role: msg.speaker === 'user' ? 'user' : 'assistant',
          content: msg.text
        })),
        {
          role: "user",
          content: inputText
        }
      ];

      const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${OPENROUTER_API_KEY}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: "openai/gpt-3.5-turbo",
          messages: messages,
          temperature: 0.7,
          max_tokens: 200
        })
      });

      const result = await response.json();
      return result.choices?.[0]?.message?.content || "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„ÙÙ‡Ù… ØªÙ…Ø§Ù…Ù‹Ø§ØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ¶ÙŠØ­ØŸ";
    } catch (error) {
      console.error("OpenRouter Error:", error);
      return "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.";
    } finally {
      setIsProcessing(false);
    }
  };

  const toggleRecording = async () => {
    if (!('webkitSpeechRecognition' in window)) {
      alert("Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ¹Ø±Ù Ø§Ù„ØµÙˆØªÙŠ");
      return;
    }
    if (isListening) {
      setIsListening(false);
      return;
    }
    setIsListening(true);
    const recognition = new window.webkitSpeechRecognition();
    recognition.lang = 'ar-SA';
    recognition.interimResults = false;

    recognition.onresult = async (event) => {
      const userText = event.results[0][0].transcript;
      setConversation(prev => [...prev, { text: userText, speaker: 'user' }]);
      const aiResponse = await queryAI(userText);
      setConversation(prev => [...prev, { text: aiResponse, speaker: 'ai' }]);
      await speakWithElevenLabs(aiResponse);
    };

    recognition.onerror = (event) => {
      console.error("Recognition error:", event.error);
      setIsListening(false);
    };

    recognition.onend = () => {
      setIsListening(false);
    };

    recognition.start();
  };

  const speakWithElevenLabs = async (text) => {
    try {
      const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM`, {
        method: 'POST',
        headers: {
          'xi-api-key': ELEVENLABS_API_KEY,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          text: text,
          voice_settings: {
            stability: 0.5,
            similarity_boost: 0.75
          }
        })
      });

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      audio.play();
      audio.onended = () => URL.revokeObjectURL(audioUrl);
    } catch (error) {
      console.error("ElevenLabs Error:", error);
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'ar-SA';
      window.speechSynthesis.speak(utterance);
    }
  };

  return (
    <group
      ref={groupRef}
      position={[1, 0.3, 0.6]}
      rotation={[0, Math.PI, 0]}
      scale={[0.5, 0.5, 0.5]}
      onClick={toggleRecording}
    >
      <primitive object={scene} />
      {isListening && (
        <mesh position={[0, 1.5, 0]}>
          <sphereGeometry args={[0.2, 16, 16]} />
          <meshBasicMaterial color="red" />
        </mesh>
      )}
      {isProcessing && (
        <mesh position={[0, 1.8, 0]}>
          <boxGeometry args={[0.2, 0.2, 0.2]} />
          <meshBasicMaterial color="yellow" />
        </mesh>
      )}
    </group>
  );
};

export default VoiceAssistant;
